{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,Conv2D,Conv1D,MaxPooling1D,  Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D,GlobalMaxPooling1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"C:/Users/Harvey/Desktop/Yelp_data_set/restuarant_review_balanced.csv\"\n",
    "df = pd.read_csv(\"/home/ec2-user/Data/restuarant_review_5_label_unbalanced.csv\")\n",
    "#df = pd.read_csv(\"/usr4/cs542sp/zzjiang/Data/restuarant_review_5_label_unbalanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "train = df.sample(frac = 0.8,random_state = 200)\n",
    "test = df.drop(train.index)\n",
    "train.loc[:,'stars'] -= 1\n",
    "print(np.unique(train['stars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train['Processed_Reviews'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train['Processed_Reviews'])\n",
    "\n",
    "\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = to_categorical(train['stars'])\n",
    "#####################\n",
    "# Test data\n",
    "tokenizer.fit_on_texts(test['Processed_Reviews'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test['Processed_Reviews'])\n",
    "\n",
    "\n",
    "maxlen = 130\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "test.loc[:,'stars'] -=1\n",
    "y_test = test['stars']\n",
    "print(np.unique(test['stars']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Vanilla RNN with 5 labels and randomly initialized word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 128)          35200896  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 130, 64)           41216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 35,243,517\n",
      "Trainable params: 35,243,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embed_size,input_length = maxlen))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 377894 samples, validate on 94474 samples\n",
      "Epoch 1/10\n",
      "377894/377894 [==============================] - 393s 1ms/step - loss: 0.8968 - acc: 0.6103 - val_loss: 0.8454 - val_acc: 0.6331\n",
      "Epoch 2/10\n",
      "377894/377894 [==============================] - 392s 1ms/step - loss: 0.8353 - acc: 0.6374 - val_loss: 0.8209 - val_acc: 0.6421\n",
      "Epoch 3/10\n",
      "377894/377894 [==============================] - 392s 1ms/step - loss: 0.8054 - acc: 0.6505 - val_loss: 0.8121 - val_acc: 0.6469\n",
      "Epoch 4/10\n",
      "377894/377894 [==============================] - 391s 1ms/step - loss: 0.7841 - acc: 0.6610 - val_loss: 0.8010 - val_acc: 0.6507\n",
      "Epoch 5/10\n",
      "377894/377894 [==============================] - 391s 1ms/step - loss: 0.7646 - acc: 0.6691 - val_loss: 0.8017 - val_acc: 0.6515\n",
      "Epoch 6/10\n",
      "377894/377894 [==============================] - 392s 1ms/step - loss: 0.7454 - acc: 0.6782 - val_loss: 0.7982 - val_acc: 0.6516\n",
      "Epoch 7/10\n",
      " 11264/377894 [..............................] - ETA: 5:51 - loss: 0.7116 - acc: 0.6895"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "y_pred = np.argmax(prediction,axis = 1)\n",
    "y_test = np.array(y_test)\n",
    "print(y_pred[:10])\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('accuracy :{0}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Vanilla RNN with 5 labels with static pretrained word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Using pretrained glove vector\n",
    "#####################################################################\n",
    "GLOVE_DIR = \"/home/ec2-user/Data/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embed_size = 100\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embed_size,input_length = maxlen))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "y_pred = np.argmax(prediction,axis = 1)\n",
    "y_test = np.array(y_test)\n",
    "print(y_pred[:10])\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('accuracy :{0}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Vanilla RNN with 5 labels with non-static pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embed_size,input_length = maxlen))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "y_pred = np.argmax(prediction,axis = 1)\n",
    "y_test = np.array(y_test)\n",
    "print(y_pred[:10])\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('accuracy :{0}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
